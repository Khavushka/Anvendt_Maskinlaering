{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises - optimization and regularization\n",
    "\n",
    "1. Use the fashion MNIST data. Build a baseline model and evaluate its performance. Go through (one at a time) early stopping, weight regularization, dropout, and batch normalization, and see how these change the performance of your model. Which method is - in isolation - best when applied to your model?\n",
    "1. Now combine the methods, such as applying early stopping *and* dropout simultaneously. Optimally, try to work through all combinations (also of more than 2 methods).\n",
    "1. Using a validation set and what you learned in (2), aim for the best model possible (preferably *without* using convolutional layers).\n",
    "\n",
    "**Note**: You may want to use:\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/regularizers\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "1. https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Consider implementing a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I suggest you implement a function to quickly build models, \n",
    "# where you can tune size, regularization, dropout, and batch normalization (and perhaps more!)\n",
    "# You may draw inspiration from the function from the slides, but note that the large model from there may be\n",
    "# prohibitively large!\n",
    "def build_model():\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the fashion MNIST data. Build a baseline model and evaluate its performance. Go through (one at a time) early stopping, weight regularization, dropout, and batch normalization, and see how these change the performance of your model. Which method is - in isolation - best when applied to your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Now combine the methods, such as applying early stopping *and* dropout simultaneously. Optimally, try to work through all combinations (also of more than 2 methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Using a validation set and what you learned in (2), aim for the best model possible (preferably *without* using convolutional layers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
