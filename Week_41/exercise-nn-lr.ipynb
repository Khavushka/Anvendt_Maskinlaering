{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - learning rate and batch size\n",
    "\n",
    "1. Consider last exercise (i.e. the MNIST data). Suppose you are restricted to **training for only 2 epochs** but still want a good model. You recognize that finding the right learning rate is going to be very important. For this reason, you split your training data into a train and a validation set and use the validation set to find the optimal learning rate. Train a model with you optimized learning rate and evaluate it on your test data.\n",
    "1. Recognizing that the batch size is also important for training speed, you decide to extend your above analysis to also find the optimal batch size. Once again, train a model with you optimized learning rate *and* batch size and evaluate it on your test data.\n",
    "1. You have heard that momentum is important. You know that many optimizers already incorporate momentum by default, but you are now forced by your evil teacher to use SGD and otherwise repeat (1) and (2). You decide to extend your above analysis to also find the optimal momentum for SGD (see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD for how to set it). Once again, train a model with you optimized learning rate, batch size, *and* momentum and evaluate it on your test data.\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Consider last exercise (i.e. the MNIST data). Suppose you are restricted to **training for only 2 epochs** but still want a good model. You recognize that finding the right learning rate is going to be very important. For this reason, you split your training data into a train and a validation set and use the validation set to find the optimal learning rate. Train a model with you optimized learning rate and evaluate it on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# A simply way to scale data. Remember pixels are in [0, 255] so this ensures [0, 1]\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Split train data to also get val data\n",
    "x_train, x_val, y_train, y_val = \n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is (parts of) a model to get you started.\n",
    "\n",
    "It is very helpful to wrap it inside a function since you want to call it multiple times in a loop.\n",
    "\n",
    "Take note of the \"Flatten\" layer. This is important to reshape your data from (28, 28) to (784,).\n",
    "\n",
    "Alternatively, you could reshape your data (the x's). This can be done using:\n",
    "\n",
    "$\\texttt{x = x.reshape(n, 784)}$ \n",
    "\n",
    "where $n$ is the number of samples (60k for training, 10k for test).\n",
    "\n",
    "Then you don't need the Flatten layer, but remember to still specify an input shape of your first layer (i.e. 784 if you have done this reshaping).\n",
    "\n",
    "**Note**: Do feel free to experiment with the number of layers, nodes per layer, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(learning_rate):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(??, activation=??),\n",
    "        tf.keras.layers.Dense(??, activation=??),\n",
    "        tf.keras.layers.Dense(??, activation=??),\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.??(lr=learning_rate)\n",
    "    model.compile(\n",
    "        loss=??,\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(??) # insert desired learning rate\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [] # must be positive floats. Default depends on optimizer\n",
    "\n",
    "results = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    model = build_model(learning_rate)\n",
    "    model.fit(??) # you may want to use verbose=0 here to not get spammed. Also remember to use epochs=2\n",
    "    loss, acc = model.evaluate(??)\n",
    "    results.append((acc, learning_rate))\n",
    "    \n",
    "results = pd.DataFrame(results, columns=['Accuracy', 'Learning rate'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['Accuracy'] == results['Accuracy'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate final model.\n",
    "# Remember to use both train and val data for training for best performance! \n",
    "# Similar to what we have done in all the other exercises/assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Recognizing that the batch size is also important for training speed, you decide to extend your above analysis to also find the optimal batch size. Once again, train a model with you optimized learning rate *and* batch size and evaluate it on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [] # must be positive floats. Default depends on optimizer\n",
    "batch_sizes = [] # # must be positive ints. Default is 32\n",
    "\n",
    "results = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        model = build_model(learning_rate)\n",
    "        model.fit(??) # remember to pass in batch_size here! Also remember to use epochs=2\n",
    "        loss, acc = model.evaluate(??)\n",
    "        results.append((acc, learning_rate, batch_size))\n",
    "    \n",
    "results = pd.DataFrame(results, columns=['Accuracy', 'Learning rate', 'Batch size'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['Accuracy'] == results['Accuracy'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate final model.\n",
    "# Remember to use both train and val data for training for best performance! \n",
    "# Similar to what we have done in all the other exercises/assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exericse 3\n",
    "\n",
    "You have heard that momentum is important. You know that many optimizers already incorporate momentum by default, but you are now forced by your evil teacher to use SGD and otherwise repeat (1) and (2). You decide to extend your above analysis to also find the optimal momentum for SGD (see https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD for how to set it). Once again, train a model with you optimized learning rate, batch size, *and* momentum and evaluate it on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_momentum(learning_rate, momentum):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(??, activation=??),\n",
    "        tf.keras.layers.Dense(??, activation=??),\n",
    "        tf.keras.layers.Dense(??, activation=??),\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.SGD(lr=learning_rate, momentum=momentum)\n",
    "    model.compile(\n",
    "        loss=??,\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [] # must be positive floats. Default (for SGD) is 0.01\n",
    "batch_sizes = [] # # must be positive ints. Default is 32\n",
    "momentums = [] # must be in [0, 1). Default (for SGD) is 0.0\n",
    "\n",
    "results = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for momentum in momentums:\n",
    "            model = build_model_with_momentum(learning_rate, momentum=momentum)\n",
    "            model.fit(??) # Remember to use epochs=2\n",
    "            loss, acc = model.evaluate(??)\n",
    "            results.append((acc, learning_rate, batch_size, momentum))\n",
    "    \n",
    "results = pd.DataFrame(results, columns=['Accuracy', 'Learning rate', 'Batch size', 'Momentum'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['Accuracy'] == results['Accuracy'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate final model.\n",
    "# Remember to use both train and val data for training for best performance! \n",
    "# Similar to what we have done in all the other exercises/assignments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
