{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - optimizers and activation functions\n",
    "\n",
    "1. Use the **fetch_california_housing** data (remember to split your data into a train and test data). Use the five optimizers presented in class to train five neural networks (identival aside from the optimizer used). How well does the networks perform on the test set, as measured by MSE and MAE? Rank the optimizers.\n",
    "1. Select the best optimizer and use it for this exercise. Experiment with different activation functions, including at least sigmoid, tanh, and relu. Rank the activation functions you try. \n",
    "1. Using your findings, as well as experimenting with more layers, try to minimize the test MSE.\n",
    "\n",
    "**Note**: You may want to use https://www.tensorflow.org/api_docs/python/tf/keras/activations and https://www.tensorflow.org/api_docs/python/tf/keras/optimizers.\n",
    "\n",
    "**Note**: **fetch_california_housing** data source file: https://github.com/scikit-learn/scikit-learn/blob/36958fb24/sklearn/datasets/_california_housing.py#L53\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the fetch_california_housing data (remember to split your data into a train and test data). Use the five optimizers presented in class to train five neural networks (identival aside from the optimizer used). How well does the networks perform on the test set, as measured by MSE and MAE? Rank the optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 8) (4128, 8) (16512,) (4128,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "z_train = scaler.fit_transform(x_train)\n",
    "z_test = scaler.transform(x_test)\n",
    "\n",
    "print(z_train.shape, z_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small function you can use as a starting point for your network - but feel free to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(activation = 'sigmoid'):\n",
    "    your_regression_nn = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=activation, input_shape=(8,)), # input_shape=8 since 8 features\n",
    "        tf.keras.layers.Dense(1, activation='linear'), # linear is used for regression. 1 node since 1 output (pr. observation)\n",
    "        ])\n",
    "\n",
    "    return your_regression_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: Remember to use \"mse\" as your loss function! Now, it is okay to try something else, but at least do not use cross entropy (remember that is for classification.\n",
    "\n",
    "Go through each of the five optimizers covered in class and rank their performance on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "516/516 [==============================] - 1s 621us/step - loss: 0.7684 - mae: 0.6668\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 0s 621us/step - loss: 0.5734 - mae: 0.5619\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 0s 568us/step - loss: 0.5380 - mae: 0.5402\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 0s 593us/step - loss: 0.5166 - mae: 0.5283\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 0s 641us/step - loss: 0.5047 - mae: 0.5214\n",
      "129/129 [==============================] - 0s 627us/step - loss: 0.4987 - mae: 0.5113\n",
      "Test mse = 0.499, test mae = 0.511.\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "# This code I have completed for you - use it to construct to other 4 cases (i.e. for the other 4 optimizers covered in class).\n",
    "nn_sgd = build_nn()\n",
    "nn_sgd.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='mse',\n",
    "    metrics=['mae'], # to also track MAE. MSE is \"automatically\" measured since it is the loss\n",
    "    )\n",
    "nn_sgd.fit(z_train, y_train, epochs=5)\n",
    "mse, mae = nn_sgd.evaluate(z_test, y_test)\n",
    "print(f'Test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 896us/step - loss: 0.5000 - mae: 0.5140\n",
      "Optimizer = sgd, Test mse = 0.5, test mae = 0.514.\n",
      "129/129 [==============================] - 0s 662us/step - loss: 0.4631 - mae: 0.4847\n",
      "Optimizer = adam, Test mse = 0.463, test mae = 0.485.\n",
      "129/129 [==============================] - 0s 732us/step - loss: 0.9352 - mae: 0.7340\n",
      "Optimizer = adagrad, Test mse = 0.935, test mae = 0.734.\n",
      "129/129 [==============================] - 0s 605us/step - loss: 3.9368 - mae: 1.6734\n",
      "Optimizer = adadelta, Test mse = 3.937, test mae = 1.673.\n",
      "129/129 [==============================] - 0s 657us/step - loss: 0.4655 - mae: 0.4961\n",
      "Optimizer = rmsprop, Test mse = 0.465, test mae = 0.496.\n"
     ]
    }
   ],
   "source": [
    "optimizers = ['sgd','adam','adagrad','adadelta','rmsprop']\n",
    "for opt in optimizers:\n",
    "# This code I have completed for you - use it to construct to other 4 cases (i.e. for the other 4 optimizers covered in class).\n",
    "    nn_gen = build_nn()\n",
    "    nn_gen.compile(\n",
    "        optimizer=opt,\n",
    "        loss='mse',\n",
    "        metrics=['mae'], # to also track MAE. MSE is \"automatically\" measured since it is the loss\n",
    "        )\n",
    "    nn_gen.fit(z_train, y_train, epochs=5,verbose=0)\n",
    "    mse, mae = nn_gen.evaluate(z_test, y_test)\n",
    "    print(f'Optimizer = {opt}, Test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Select the best optimizer and use it for this exercise. Experiment with different activation functions, including at least sigmoid, tanh, and relu. Rank the activation functions you try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 0s 638us/step - loss: 0.4674 - mae: 0.5015\n",
      "Activation = sigmoid, Test mse = 0.467, test mae = 0.501.\n",
      "129/129 [==============================] - 0s 665us/step - loss: 0.4014 - mae: 0.4343\n",
      "Activation = relu, Test mse = 0.401, test mae = 0.434.\n",
      "129/129 [==============================] - 0s 650us/step - loss: 0.4573 - mae: 0.4881\n",
      "Activation = tanh, Test mse = 0.457, test mae = 0.488.\n",
      "129/129 [==============================] - 0s 629us/step - loss: 0.4602 - mae: 0.4830\n",
      "Activation = elu, Test mse = 0.46, test mae = 0.483.\n",
      "129/129 [==============================] - 0s 671us/step - loss: 0.4222 - mae: 0.4632\n",
      "Activation = gelu, Test mse = 0.422, test mae = 0.463.\n",
      "129/129 [==============================] - 0s 627us/step - loss: 0.4664 - mae: 0.4878\n",
      "Activation = selu, Test mse = 0.466, test mae = 0.488.\n"
     ]
    }
   ],
   "source": [
    "activations = ['sigmoid','relu','tanh','elu','gelu','selu']\n",
    "for act in activations:\n",
    "    nn_gen = build_nn(act)\n",
    "    nn_gen.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    "    )\n",
    "    nn_gen.fit(z_train, y_train, epochs=5,verbose=0)\n",
    "    mse, mae = nn_gen.evaluate(z_test, y_test)\n",
    "    print(f'Activation = {act}, Test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Using your findings, as well as experimenting with more layers, try to minimize the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to experiment a bit, but here is an example of a model with more layers\n",
    "def build_better_nn(activation):\n",
    "    your_regression_nn = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation=activation, input_shape=(8,)), # input_shape=8 since 8 features\n",
    "        tf.keras.layers.Dense(64, activation=activation),\n",
    "        tf.keras.layers.Dense(128, activation=activation),\n",
    "        tf.keras.layers.Dense(1, activation='linear'), # linear is used for regression. 1 node since 1 output\n",
    "        ])\n",
    "\n",
    "    return your_regression_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "516/516 [==============================] - 1s 678us/step - loss: 0.8118 - mae: 0.5772\n",
      "Epoch 2/25\n",
      "516/516 [==============================] - 0s 707us/step - loss: 0.3889 - mae: 0.4421\n",
      "Epoch 3/25\n",
      "516/516 [==============================] - 0s 699us/step - loss: 0.3556 - mae: 0.4224\n",
      "Epoch 4/25\n",
      "516/516 [==============================] - 0s 755us/step - loss: 0.3455 - mae: 0.4112\n",
      "Epoch 5/25\n",
      "516/516 [==============================] - 0s 706us/step - loss: 0.3304 - mae: 0.4021\n",
      "Epoch 6/25\n",
      "516/516 [==============================] - 0s 680us/step - loss: 0.3108 - mae: 0.3915\n",
      "Epoch 7/25\n",
      "516/516 [==============================] - 0s 735us/step - loss: 0.3054 - mae: 0.3859\n",
      "Epoch 8/25\n",
      "516/516 [==============================] - 0s 678us/step - loss: 0.3018 - mae: 0.3821\n",
      "Epoch 9/25\n",
      "516/516 [==============================] - 0s 690us/step - loss: 0.3103 - mae: 0.3824\n",
      "Epoch 10/25\n",
      "516/516 [==============================] - 0s 733us/step - loss: 0.2918 - mae: 0.3748\n",
      "Epoch 11/25\n",
      "516/516 [==============================] - 0s 700us/step - loss: 0.2900 - mae: 0.3717\n",
      "Epoch 12/25\n",
      "516/516 [==============================] - 0s 688us/step - loss: 0.2849 - mae: 0.3691\n",
      "Epoch 13/25\n",
      "516/516 [==============================] - 0s 696us/step - loss: 0.2828 - mae: 0.3676\n",
      "Epoch 14/25\n",
      "516/516 [==============================] - 0s 703us/step - loss: 0.2763 - mae: 0.3634\n",
      "Epoch 15/25\n",
      "516/516 [==============================] - 0s 692us/step - loss: 0.2735 - mae: 0.3619\n",
      "Epoch 16/25\n",
      "516/516 [==============================] - 0s 704us/step - loss: 0.2729 - mae: 0.3604\n",
      "Epoch 17/25\n",
      "516/516 [==============================] - 0s 686us/step - loss: 0.2709 - mae: 0.3570\n",
      "Epoch 18/25\n",
      "516/516 [==============================] - 0s 742us/step - loss: 0.2662 - mae: 0.3548\n",
      "Epoch 19/25\n",
      "516/516 [==============================] - 0s 692us/step - loss: 0.2662 - mae: 0.3529\n",
      "Epoch 20/25\n",
      "516/516 [==============================] - 0s 692us/step - loss: 0.2633 - mae: 0.3525\n",
      "Epoch 21/25\n",
      "516/516 [==============================] - 0s 681us/step - loss: 0.2664 - mae: 0.3521\n",
      "Epoch 22/25\n",
      "516/516 [==============================] - 0s 685us/step - loss: 0.2603 - mae: 0.3495\n",
      "Epoch 23/25\n",
      "516/516 [==============================] - 0s 707us/step - loss: 0.2585 - mae: 0.3476\n",
      "Epoch 24/25\n",
      "516/516 [==============================] - 0s 706us/step - loss: 0.2552 - mae: 0.3462\n",
      "Epoch 25/25\n",
      "516/516 [==============================] - 0s 728us/step - loss: 0.2526 - mae: 0.3437\n",
      "129/129 [==============================] - 0s 638us/step - loss: 0.2846 - mae: 0.3495\n",
      "Final model test mse = 0.285, test mae = 0.35.\n"
     ]
    }
   ],
   "source": [
    "nn_final = build_better_nn(\"relu\")\n",
    "nn_final.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    "    )\n",
    "nn_final.fit(z_train, y_train, epochs=25)\n",
    "mse, mae = nn_final.evaluate(z_test, y_test)\n",
    "print(f'Final model test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('amlfall22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3cedec8935a2c28d6fd602c3007747750e2af1c4c937c29fac0d323bf1b544b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
