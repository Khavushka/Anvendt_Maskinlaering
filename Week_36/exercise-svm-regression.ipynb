{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - SVM for regression\n",
    "\n",
    "1. Standardize the data and implement a linear, polynomial, and RBF SVM. What is the performance (MSE) of each model now? Is the linear model still best?\n",
    "1. Try to split your training data (again using $\\texttt{train_test_split}$) to obtain a validation set. Try to optimize the performance of your model on the validation data, focusing particularly on regularization ($C$). Can you achieve test MSE below 10 (this is not trivial!)? In the original paper, they achieve an MSE of 7.2 (although it is not directly comparable). Remember to use standadization!\n",
    "\n",
    "**Note**: Large values of $C$ may be VERY slow to fit (for some of the models)! Try not to go too extreme, as your code may crash.\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston # NOTE how we use the Boston data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error # NOTE how we use a new metric!\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "X_train, X_test, y_train, y_test = \n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Standardize the data and implement a linear, polynomial, and RBF SVM. What is the performance (MSE) of each model now? Is the linear model still best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will call the standardized X for Z.\n",
    "# It it important, that you only fit your scaler on your training and not your test data!\n",
    "# This is akin to when you fit your model - you do not want to \"peak\" at the test data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Z_train = \n",
    "Z_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVM\n",
    "svm_linear = \n",
    "svm_linear.fit(Z_train, y_train) # remember fit to Z_train, NOT X_train\n",
    "\n",
    "# ... and predicting\n",
    "y_test_hat_linear = svm_linear.predict(Z_test) # And Z_test here instead of X_test\n",
    "mse_linear = mean_squared_error(y_test_hat_linear, y_test)\n",
    "print(f'Linear SVM achieved {round(mse_linear, 3)} MSE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial SVM (you decide degree)\n",
    "svm_poly = svm.SVR(kernel='poly', degree=??)\n",
    "svm_poly.fit(??)\n",
    "\n",
    "# ... and predicting\n",
    "y_test_hat_poly = \n",
    "mse_poly = \n",
    "print(f'Polynomial SVM achieved {round(mse_poly, 3)} MSE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = \n",
    "svm_rbf.fit(??)\n",
    "\n",
    "# ... and predicting\n",
    "y_test_hat_rbf = \n",
    "mse_rbf = \n",
    "print(f'RBF SVM achieved {round(mse_rbf, 3)} MSE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Try to split your training data (again using $\\texttt{train_test_split}$) to obtain a validation set. Try to optimize the performance of your model on the validation data, focusing particularly on regularization ($C$). Can you achieve test MSE below 10 (this is not trivial!)? In the original paper, they achieve an MSE of 7.2 (although it is not directly comparable). Remember to use standadization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by splitting the train data to also obtain validation data\n",
    "Z_train, Z_val, y_train, y_val = \n",
    "print(Z_train.shape, Z_val.shape, Z_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now try different values of kernels, C, epsilon, as well as any other settings you want to tune\n",
    "\n",
    "kernels = [] # input values seperated by \",\".\n",
    "Cs = [] # input values seperated by \",\".\n",
    "epsilons = [] # input values seperated by \",\".\n",
    "\n",
    "results = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    for C in Cs:\n",
    "        for epsilon in epsilons:\n",
    "            svm_current = svm.SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "            svm_current.fit(Z_train, y_train)\n",
    "            y_val_hat = svm_current.predict(Z_val)\n",
    "            mse = mean_squared_error(y_val_hat, y_val)\n",
    "\n",
    "            results.append([mse, kernel, C, epsilon])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['MSE', 'Kernel', 'C', 'epsilon']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best parameters.\n",
    "results[results['MSE'] == results['MSE'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your final model\n",
    "svm_optimized = svm.SVR(kernel=??, C=??, epsilon=??)\n",
    "\n",
    "# Use both training and validation data to fit it (np.concatenate \"stacks\" the array like rbind in R)\n",
    "svm_optimized.fit(np.concatenate([Z_train, Z_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "# Predict on test data\n",
    "y_val_hat_optimized = svm_optimized.predict(Z_test)\n",
    "\n",
    "# Obtain and check accuracy on test data\n",
    "accuracy_optimized = mean_squared_error(y_val_hat_optimized, y_test)\n",
    "print(f'Optimized SVM achieved {round(accuracy_optimized, 3)} MSE.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
