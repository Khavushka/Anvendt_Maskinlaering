{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Time to experiment with activation functions and optimizers! And now we're at it, let's use this as an introduction to regression using neural networks as well.\n",
    "\n",
    "1. Use the **fetch_california_housing** data (remember to split your data into a train and test data). Use the five optimizers presented in class to train five neural networks (identival aside from the optimizer used). How well does the networks perform on the test set, as measured by MSE and MAE? Rank the optimizers.\n",
    "1. Select the best optimizer and use it for this exercise. Experiment with different activation functions, including at least sigmoid, tanh, and relu. Rank the activation functions you try. \n",
    "1. Using your findings, as well as experimenting with more layers, try to minimize the test MSE.\n",
    "\n",
    "**Note**: You may want to use https://www.tensorflow.org/api_docs/python/tf/keras/activations and https://www.tensorflow.org/api_docs/python/tf/keras/optimizers.\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the **fetch_california_housing** data (remember to split your data into a train and test data). Use the five optimizers presented in class to train five neural networks (identival aside from the optimizer used). How well does the networks perform on the test set, as measured by MSE and MAE? Rank the optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 8) (4128, 8) (16512,) (4128,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x, y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "z_train = scaler.fit_transform(x_train)\n",
    "z_test = scaler.transform(x_test)\n",
    "\n",
    "print(z_train.shape, z_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small function you can use as a starting point for your network - but feel free to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(activation = 'sigmoid'):\n",
    "    your_regression_nn = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=activation, input_shape=(8,)), # input_shape=8 since 8 features\n",
    "        tf.keras.layers.Dense(1, activation='linear'), # linear is used for regression. 1 node since 1 output (pr. observation)\n",
    "        ])\n",
    "\n",
    "    return your_regression_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: Remember to use \"mse\" as your loss function! Now, it is okay to try something else, but at least do not use cross entropy (remember that is for classification.\n",
    "\n",
    "Go through each of the five optimizers covered in class and rank their performance on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "516/516 [==============================] - 2s 3ms/step - loss: 0.7819 - mae: 0.6717\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5831 - mae: 0.5667\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5500 - mae: 0.5477\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.5268 - mae: 0.5348\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.5134 - mae: 0.5268\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 0.5065 - mae: 0.5231\n",
      "Test mse = 0.507, test mae = 0.523.\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "# This code I have completed for you - use it to construct to other 4 cases (i.e. for the other 4 optimizers covered in class).\n",
    "nn_sgd = build_nn()\n",
    "nn_sgd.compile(\n",
    "    optimizer='SGD',\n",
    "    loss='mse',\n",
    "    metrics=['mae'], # to also track MAE. MSE is \"automatically\" measured since it is the loss\n",
    "    )\n",
    "nn_sgd.fit(z_train, y_train, epochs=5)\n",
    "mse, mae = nn_sgd.evaluate(z_test, y_test)\n",
    "print(f'Test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavere tal indikerer bedre præstation\n",
    "\n",
    "<!-- En for høj værdi kan få parametrene til at ændre sig hurtigt, hvilket gør det vanskeligt at konvergere. -->\n",
    "\n",
    "<!-- En for lav værdi betyder, at netværket konvergerer meget langsomt, og det er også mere tilbøjeligt til at sidde fast i dårlige lokale minima. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Select the best optimizer and use it for this exercise. Experiment with different activation functions, including at least sigmoid, tanh, and relu. Rank the activation functions you try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "516/516 [==============================] - 2s 2ms/step - loss: 0.6529 - mae: 0.5782\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4889 - mae: 0.5133\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 2s 3ms/step - loss: 0.4774 - mae: 0.5069\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 3ms/step - loss: 0.4695 - mae: 0.5004\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4637 - mae: 0.4965\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 0.4615 - mae: 0.4928\n",
      "Test mse = 0.461, test mae = 0.493.\n"
     ]
    }
   ],
   "source": [
    "# Example of using tanh\n",
    "nn_tanh = build_nn('tanh')\n",
    "nn_tanh.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    "    )\n",
    "nn_tanh.fit(z_train, y_train, epochs=5)\n",
    "mse, mae = nn_tanh.evaluate(z_test, y_test)\n",
    "print(f'Test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Using your findings, as well as experimenting with more layers, try to minimize the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to experiment a bit, but here is an example of a model with more layers\n",
    "def build_better_nn(activation):\n",
    "    your_regression_nn = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation=activation, input_shape=(8,)), # input_shape=8 since 8 features\n",
    "        tf.keras.layers.Dense(64, activation=activation),\n",
    "        tf.keras.layers.Dense(128, activation=activation),\n",
    "        tf.keras.layers.Dense(1, activation='linear'), # linear is used for regression. 1 node since 1 output\n",
    "        ])\n",
    "\n",
    "    return your_regression_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "516/516 [==============================] - 3s 2ms/step - loss: 0.8708 - mae: 0.7047\n",
      "Epoch 2/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4760 - mae: 0.5022\n",
      "Epoch 3/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4333 - mae: 0.4749\n",
      "Epoch 4/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4224 - mae: 0.4681\n",
      "Epoch 5/5\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4122 - mae: 0.4618\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 0.4278 - mae: 0.4508\n",
      "Final model test mse = 0.428, test mae = 0.451.\n"
     ]
    }
   ],
   "source": [
    "nn_final = build_better_nn('sigmoid')\n",
    "nn_final.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    "    )\n",
    "nn_final.fit(z_train, y_train, epochs=5)\n",
    "mse, mae = nn_final.evaluate(z_test, y_test)\n",
    "print(f'Final model test mse = {round(mse, 3)}, test mae = {round(mae, 3)}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
