{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - DTs for regression\n",
    "\n",
    "1. Use the `./data/HousingData.csv` data (remember to split your data into a train and test data). Using your training and validation data, optimize the parameters of your DT. How well does your optimized model perform on the test data?\n",
    "1. (Optional/bonus): Try to perform standardization to your data. Does it improve your model? Further, try to select only the 5 most important features. Does it improve the performance of your model?\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
      "5  0.02985   0.0   2.18   0.0  0.458  6.430  58.7  6.0622    3  222     18.7   \n",
      "\n",
      "        B  LSTAT  MEDV  \n",
      "0  396.90   4.98  24.0  \n",
      "1  396.90   9.14  21.6  \n",
      "2  392.83   4.03  34.7  \n",
      "3  394.63   2.94  33.4  \n",
      "5  394.12   5.21  28.7  \n",
      "\n",
      "The shape of train, validation and test sets are:\n",
      "(252, 13) (63, 13) (79, 13) (252,) (63,) (79,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "\n",
    "data = './data/HousingData.csv'\n",
    "raw_df = pd.read_csv(data).dropna()\n",
    "\n",
    "print(raw_df.head())\n",
    "\n",
    "# Create a copy of the DataFrame with column names\n",
    "df_copy = raw_df.copy()\n",
    "\n",
    "# Separate the target variable (y) and features (X)\n",
    "y = df_copy['MEDV']  # Replace 'TargetColumn' with your actual target column name\n",
    "\n",
    "# MEDV - median value of owner occupied\n",
    "X = df_copy.drop(columns=['MEDV'])  # Remove the target column\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use `train_test_split` to split your train data into a train and a validation  set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\nThe shape of train, validation and test sets are:\")\n",
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the `./data/HousingData.csv` data (remember to split your data into a train and test data). Using your training and validation data, optimize the parameters of your DT. How well does your optimized model perform on the test data? Is it better than your optimized SVM for the same data (the third exercise from last week)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MSE  min_samples_split  min_samples_leaf  max_features\n",
      "0   24.098091                  2                 5             2\n",
      "1   22.470092                  2                 5             5\n",
      "2   23.613533                  2                 5            10\n",
      "3   27.777024                  2                10             2\n",
      "4   26.611791                  2                10             5\n",
      "5   19.295205                  2                10            10\n",
      "6   41.788484                  2                15             2\n",
      "7   26.515413                  2                15             5\n",
      "8   21.356843                  2                15            10\n",
      "9   37.637093                  4                 5             2\n",
      "10  30.142498                  4                 5             5\n",
      "11  20.315579                  4                 5            10\n",
      "12  27.244707                  4                10             2\n",
      "13  31.695888                  4                10             5\n",
      "14  21.521898                  4                10            10\n",
      "15  44.998833                  4                15             2\n",
      "16  25.785671                  4                15             5\n",
      "17  24.694536                  4                15            10\n",
      "18  28.077303                  6                 5             2\n",
      "19  18.015398                  6                 5             5\n",
      "20  13.662864                  6                 5            10\n",
      "21  28.099405                  6                10             2\n",
      "22  25.855312                  6                10             5\n",
      "23  19.318592                  6                10            10\n",
      "24  27.506982                  6                15             2\n",
      "25  32.548306                  6                15             5\n",
      "26  20.326181                  6                15            10\n",
      "27  43.048829                 12                 5             2\n",
      "28  32.006592                 12                 5             5\n",
      "29  22.592703                 12                 5            10\n",
      "30  21.042844                 12                10             2\n",
      "31  25.907103                 12                10             5\n",
      "32  20.602282                 12                10            10\n",
      "33  20.620549                 12                15             2\n",
      "34  30.937773                 12                15             5\n",
      "35  15.344754                 12                15            10\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "min_samples_split_list = [2, 4, 6, 12] # input values seperated by \",\".\n",
    "min_samples_leaf_list = [5, 10, 15] # input values seperated by \",\".\n",
    "max_features_list = [2, 5, 10] # input values seperated by \",\".\n",
    "\n",
    "results = []\n",
    "\n",
    "for min_samples_split in min_samples_split_list:\n",
    "    for min_samples_leaf in min_samples_leaf_list:\n",
    "        for max_features in max_features_list:\n",
    "            dt_current = tree.DecisionTreeRegressor(min_samples_split=min_samples_split,\n",
    "                                                    min_samples_leaf=min_samples_leaf,\n",
    "                                                    max_features=max_features)\n",
    "                                                    \n",
    "            dt_current.fit(X_train, y_train)\n",
    "            y_val_hat = dt_current.predict(X_val)\n",
    "            mse = mean_squared_error(y_val, y_val_hat)\n",
    "\n",
    "            results.append([mse, min_samples_split, min_samples_leaf, max_features])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['MSE', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MSE  min_samples_split  min_samples_leaf  max_features\n",
      "20  13.662864                  6                 5            10\n"
     ]
    }
   ],
   "source": [
    "# Extract best parameters.\n",
    "results[results['MSE'] == results['MSE'].max()]\n",
    "\n",
    "print(results[results['MSE'] == results['MSE'].min()])\n",
    "max_features_optimal = results.loc[results['MSE'].idxmin()]['max_features'].astype(int)\n",
    "min_samples_split_optimal = results.loc[results['MSE'].idxmin()]['min_samples_split'].astype(int)\n",
    "min_samples_leaf_optimal = results.loc[results['MSE'].idxmin()]['min_samples_leaf'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized DT achieved MSE = 38.76. - lower values indicating better performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khavu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize your final model\n",
    "dt_optimized = tree.DecisionTreeRegressor(\n",
    "        max_features=max_features_optimal,\n",
    "        min_samples_split=min_samples_split_optimal,\n",
    "        min_samples_leaf=min_samples_leaf_optimal,\n",
    "    )\n",
    "\n",
    "# Use both training and validation data to fit it (np.concatenate \"stacks\" the array like rbind in R)\n",
    "dt_optimized.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "# Predict on test data\n",
    "y_test_hat_optimized = dt_optimized.predict(X_test)\n",
    "\n",
    "# Obtain and check mse on test data\n",
    "mse_optimized = mean_squared_error(y_test, y_test_hat_optimized)\n",
    "print(f'Optimized DT achieved MSE = {round(mse_optimized, 2)}. - lower values indicating better performance.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "(Optional/bonus): Try to perform standardization to your data. Does it improve your model? Further, try to select only the 5 most important features. Does it improve the performance of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252, 13) (63, 13) (79, 13) (252,) (63,) (79,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both train and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled.shape, X_val_scaled.shape, X_test_scaled.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MSE  min_samples_split  min_samples_leaf  max_features\n",
      "0   27.167080                  2                 5             2\n",
      "1   12.427780                  2                 5             5\n",
      "2   21.916429                  2                 5            10\n",
      "3   28.801231                  2                10             2\n",
      "4   20.296075                  2                10             5\n",
      "5   11.944843                  2                10            10\n",
      "6   21.303572                  2                15             2\n",
      "7   32.932800                  2                15             5\n",
      "8   24.305137                  2                15            10\n",
      "9   25.537715                  4                 5             2\n",
      "10  22.110094                  4                 5             5\n",
      "11  12.030186                  4                 5            10\n",
      "12  18.011641                  4                10             2\n",
      "13  15.676869                  4                10             5\n",
      "14  18.541349                  4                10            10\n",
      "15  30.662696                  4                15             2\n",
      "16  16.756600                  4                15             5\n",
      "17  19.775186                  4                15            10\n",
      "18  25.771352                  6                 5             2\n",
      "19  18.446251                  6                 5             5\n",
      "20  24.174663                  6                 5            10\n",
      "21  25.167166                  6                10             2\n",
      "22  20.515298                  6                10             5\n",
      "23  18.505115                  6                10            10\n",
      "24  24.577000                  6                15             2\n",
      "25  22.585276                  6                15             5\n",
      "26  20.903130                  6                15            10\n",
      "27  24.092675                 12                 5             2\n",
      "28  14.663176                 12                 5             5\n",
      "29  23.125621                 12                 5            10\n",
      "30  31.054020                 12                10             2\n",
      "31  15.108899                 12                10             5\n",
      "32  19.059638                 12                10            10\n",
      "33  43.554592                 12                15             2\n",
      "34  27.824969                 12                15             5\n",
      "35  20.309826                 12                15            10\n"
     ]
    }
   ],
   "source": [
    "# COPY FRA SOLUTION\n",
    "results_scaled = []\n",
    "for min_samples_split in min_samples_split_list:\n",
    "    for min_samples_leaf in min_samples_leaf_list:\n",
    "        for max_features in max_features_list:\n",
    "            dt_current = tree.DecisionTreeRegressor(min_samples_split=min_samples_split,\n",
    "                                                    min_samples_leaf=min_samples_leaf,\n",
    "                                                    max_features=max_features)\n",
    "                                                    \n",
    "            dt_current.fit(X_train_scaled, y_train)\n",
    "            y_val_hat = dt_current.predict(X_val_scaled)\n",
    "            mse = mean_squared_error(y_val, y_val_hat)\n",
    "            results_scaled.append([mse, min_samples_split, min_samples_leaf, max_features])\n",
    "\n",
    "results_scaled = pd.DataFrame(results_scaled)\n",
    "results_scaled.columns = ['MSE', 'min_samples_split', 'min_samples_leaf', 'max_features']\n",
    "print(results_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         MSE  min_samples_split  min_samples_leaf  max_features\n",
      "5  11.944843                  2                10            10\n"
     ]
    }
   ],
   "source": [
    "# Extract best parameters.\n",
    "print(results_scaled[results_scaled['MSE'] == results_scaled['MSE'].min()])\n",
    "max_features_optimal = results_scaled.loc[results_scaled['MSE'].idxmin()]['max_features'].astype(int)\n",
    "min_samples_split_optimal = results_scaled.loc[results_scaled['MSE'].idxmin()]['min_samples_split'].astype(int)\n",
    "min_samples_leaf_optimal = results_scaled.loc[results_scaled['MSE'].idxmin()]['min_samples_leaf'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RF achieved MSE = 32.9. - lower values indicating better performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khavu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# Initialize your final model\n",
    "final_dt =  tree.DecisionTreeRegressor(\n",
    "                max_features=max_features_optimal,\n",
    "                min_samples_split=min_samples_split_optimal,\n",
    "                min_samples_leaf=min_samples_leaf_optimal,\n",
    "                )   \n",
    "\n",
    "# Use both training and validation data to fit it (np.concatenate \"stacks\" the array like rbind in R)\n",
    "final_dt.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "# Predict on test data\n",
    "y_test_hat_optimized = final_dt.predict(X_test)\n",
    "\n",
    "# Obtain and check mse on test data\n",
    "rf_optimized = mean_squared_error(y_test, y_test_hat_optimized)\n",
    "print(f'Optimized RF achieved MSE = {round(rf_optimized, 1)}. - lower values indicating better performance.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
