{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Boosting for classification\n",
    "\n",
    "1. Use the **load_breast_cancer** data (remember to split your data into a train, validation, and test data). Using your training and validation data, optimize the parameters of your GradientBoostingClassifier. How well does your optimized model perform on the test data?\n",
    "1. Implement an RF model. How well do they perform on the test data? Try to \"vote\" using boosting and RF and select the class with the most votes. How well does your ensemble of all three models perform?\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 30) (91, 30) (114, 30) (364,) (91,) (114,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import ensemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use `train_test_split` to split your train data into a train and a validation  set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the **load_breast_cancer** data (remember to split your data into a train, validation, and test data). Using your training and validation data, optimize the parameters of your GradientBoostingClassifier. How well does your optimized model perform on the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by ensuring we can just run an GBT without any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting with default settings has validation accuracy of 95.6%.\n"
     ]
    }
   ],
   "source": [
    "gbt_current = ensemble.GradientBoostingClassifier()\n",
    "gbt_current.fit(X_train, y_train)\n",
    "y_val_hat = gbt_current.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_val_hat)\n",
    "\n",
    "print(f'Boosting with default settings has validation accuracy of {round(acc * 100, 2)}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy  n_estimators  min_samples_split  min_samples_leaf\n",
      "0   0.989011            50                 15                 5\n",
      "1   1.000000            50                 15                10\n",
      "2   1.000000            50                 15                15\n",
      "3   1.000000            50                 20                 5\n",
      "4   1.000000            50                 20                10\n",
      "5   1.000000            50                 20                15\n",
      "6   1.000000            50                 25                 5\n",
      "7   1.000000            50                 25                10\n",
      "8   1.000000            50                 25                15\n",
      "9   1.000000           200                 15                 5\n",
      "10  1.000000           200                 15                10\n",
      "11  1.000000           200                 15                15\n",
      "12  1.000000           200                 20                 5\n",
      "13  1.000000           200                 20                10\n",
      "14  1.000000           200                 20                15\n",
      "15  1.000000           200                 25                 5\n",
      "16  1.000000           200                 25                10\n",
      "17  1.000000           200                 25                15\n",
      "18  1.000000           500                 15                 5\n",
      "19  1.000000           500                 15                10\n",
      "20  1.000000           500                 15                15\n",
      "21  1.000000           500                 20                 5\n",
      "22  1.000000           500                 20                10\n",
      "23  1.000000           500                 20                15\n",
      "24  1.000000           500                 25                 5\n",
      "25  1.000000           500                 25                10\n",
      "26  1.000000           500                 25                15\n"
     ]
    }
   ],
   "source": [
    "# Remember you can try other stuff than these specific parameters.\n",
    "# Just here to get you started!\n",
    "\n",
    "n_estimators_list = [50,200,500]\n",
    "min_samples_split_list = [15,20,25]\n",
    "min_samples_leaf_list = [5,10,15]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for min_samples_split in min_samples_split_list:\n",
    "        for min_samples_leaf in min_samples_leaf_list:\n",
    "            gbt_current = ensemble.GradientBoostingClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                )\n",
    "            gbt_current.fit(X_train, y_train)\n",
    "            y_val_hat = gbt_current.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_val_hat)\n",
    "\n",
    "            results.append([acc, n_estimators, min_samples_split, min_samples_leaf])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Accuracy', 'n_estimators', 'min_samples_split', 'min_samples_leaf']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best parameters.\n",
    "n_estimators_optimal = results.loc[results['Accuracy'].idxmax()]['n_estimators'].astype(int)\n",
    "min_samples_split_optimal = results.loc[results['Accuracy'].idxmax()]['min_samples_split'].astype(int)\n",
    "min_samples_leaf_optimal = results.loc[results['Accuracy'].idxmax()]['min_samples_leaf'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting with optimal settings has a test accuracy of 97.37%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize your final model\n",
    "gbt_optimal = ensemble.GradientBoostingClassifier(\n",
    "                n_estimators=n_estimators_optimal,\n",
    "                min_samples_split=min_samples_split_optimal,\n",
    "                min_samples_leaf=min_samples_leaf_optimal,\n",
    "                )            \n",
    "\n",
    "# Use both training and validation data to fit it using np.concatenate (np.concatenate \"stacks\" the array like rbind in R)\n",
    "X_train = np.concatenate((X_train,X_val))\n",
    "y_train = np.concatenate((y_train,y_val))\n",
    "#y_train\n",
    "gbt_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_hat = gbt_current.predict(X_test)\n",
    "\n",
    "# Obtain and check accuracy on test data\n",
    "acc = accuracy_score(y_test, y_test_hat)\n",
    "print(f'Boosting with optimal settings has a test accuracy of {round(acc * 100, 2)}%.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
