{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Boosting for classification\n",
    "\n",
    "1. Use the **load_breast_cancer** data (remember to split your data into a train, validation, and test data). Using your training and validation data, optimize the parameters of your GradientBoostingClassifier. How well does your optimized model perform on the test data?\n",
    "1. Implement an RF and a SVM and use these as well (**note**: you may want to perform standardization for the SVM). How well do they perform on the test data? Try to \"vote\" using all three models (boosting, RF, and SVM) and select the class with the most votes. How well does your ensemble of all three models perform?\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 30) (91, 30) (114, 30) (364,) (91,) (114,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import ensemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use `train_test_split` to split your train data into a train and a validation  set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the **load_breast_cancer** data (remember to split your data into a train, validation, and test data). Using your training and validation data, optimize the parameters of your GradientBoostingClassifier. How well does your optimized model perform on the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by ensuring we can just run an GBT without any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting with default settings has validation accuracy of 95.6%.\n"
     ]
    }
   ],
   "source": [
    "gbt_current = ensemble.GradientBoostingClassifier()\n",
    "gbt_current.fit(X_train, y_train)\n",
    "y_val_hat = gbt_current.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_val_hat)\n",
    "\n",
    "print(f'Boosting with default settings has validation accuracy of {round(acc * 100, 2)}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy  n_estimators  min_samples_split  min_samples_leaf\n",
      "0   0.912088             5                 15                 5\n",
      "1   0.923077             5                 15                10\n",
      "2   0.923077             5                 15                20\n",
      "3   0.912088             5                 20                 5\n",
      "4   0.934066             5                 20                10\n",
      "5   0.923077             5                 20                20\n",
      "6   0.912088             5                 25                 5\n",
      "7   0.901099             5                 25                10\n",
      "8   0.923077             5                 25                20\n",
      "9   0.945055            20                 15                 5\n",
      "10  0.934066            20                 15                10\n",
      "11  0.934066            20                 15                20\n",
      "12  0.945055            20                 20                 5\n",
      "13  0.934066            20                 20                10\n",
      "14  0.934066            20                 20                20\n",
      "15  0.945055            20                 25                 5\n",
      "16  0.934066            20                 25                10\n",
      "17  0.945055            20                 25                20\n",
      "18  0.956044            50                 15                 5\n",
      "19  0.956044            50                 15                10\n",
      "20  0.956044            50                 15                20\n",
      "21  0.967033            50                 20                 5\n",
      "22  0.956044            50                 20                10\n",
      "23  0.956044            50                 20                20\n",
      "24  0.967033            50                 25                 5\n",
      "25  0.956044            50                 25                10\n",
      "26  0.956044            50                 25                20\n"
     ]
    }
   ],
   "source": [
    "# Remember you can try other stuff than these specific parameters.\n",
    "# Just here to get you started!\n",
    "\n",
    "n_estimators_list = [5, 20, 50]\n",
    "min_samples_split_list = [15, 20, 25]\n",
    "min_samples_leaf_list = [5, 10, 20]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for min_samples_split in min_samples_split_list:\n",
    "        for min_samples_leaf in min_samples_leaf_list:\n",
    "            gbt_current = ensemble.GradientBoostingClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                )\n",
    "            gbt_current.fit(X_train, y_train)\n",
    "            y_val_hat = gbt_current.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_val_hat)\n",
    "\n",
    "            results.append([acc, n_estimators, min_samples_split, min_samples_leaf])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Accuracy', 'n_estimators', 'min_samples_split', 'min_samples_leaf']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best parameters.\n",
    "# results[results['Accuracy'] == results['Accuracy'].max()]\n",
    "n_estimators_optimal = results.loc[results['Accuracy'].idxmax()]['n_estimators'].astype(int)\n",
    "min_samples_split_optimal = results.loc[results['Accuracy'].idxmax()]['min_samples_split'].astype(int)\n",
    "min_samples_leaf_optimal = results.loc[results['Accuracy'].idxmax()]['min_samples_leaf'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting with optimal settings has a test accuracy of 93.86%.\n"
     ]
    }
   ],
   "source": [
    "# Initialize your final model\n",
    "gbt_optimal = ensemble.GradientBoostingClassifier(\n",
    "                n_estimators=n_estimators_optimal,\n",
    "                min_samples_split=min_samples_split_optimal,\n",
    "                min_samples_leaf=min_samples_leaf_optimal,\n",
    "                )            \n",
    "\n",
    "# Use both training and validation data to fit it using np.concatenate (np.concatenate \"stacks\" the array like rbind in R)\n",
    "X_train = np.concatenate((X_train,X_val))\n",
    "y_train = np.concatenate((y_train,y_val))\n",
    "#y_train\n",
    "gbt_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_hat_gbt = gbt_current.predict(X_test)\n",
    "\n",
    "# Obtain and check accuracy on test data\n",
    "acc = accuracy_score(y_test_hat_gbt, y_test)\n",
    "print(f'Boosting with optimal settings has a test accuracy of {round(acc * 100, 2)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Implement an RF and a SVM and use these as well (**note**: you may want to perform standardization for the SVM). How well do they perform on the test data? Try to \"vote\" using all three models (boosting, RF, and SVM) and select the class with the most votes. How well does your ensemble of all three models perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "# Scale your data\n",
    "scaler = StandardScaler()\n",
    "Z_train = scaler.fit_transform(X_train)\n",
    "Z_val = scaler.transform(X_val)\n",
    "Z_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.967033</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.967033</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  n_estimators  min_samples_split  min_samples_leaf\n",
       "21  0.967033            50                 20                 5\n",
       "24  0.967033            50                 25                 5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may want to optimize the settings if you want.\n",
    "# Then, you can do it here.\n",
    "# You can/may want to do this both for the RF and the SVM.\n",
    "\n",
    "results[results['Accuracy'] == results['Accuracy'].max()]\n",
    "# print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest achieved test accuracy of 95.61%.\n",
      "SVM achieved test accuracy of 98.25%.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize your final models\n",
    "gb_final = RandomForestClassifier(\n",
    "    n_estimators = 5,\n",
    "    random_state=42,\n",
    "    # min_samples_split = 5, \n",
    "    # min_samples_leaf = 10\n",
    ")\n",
    "# Use both training and validation data to fit them using np.concatenate (np.concatenate \"stacks\" the array like rbind in R)\n",
    "gb_final.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "# Predict on test data\n",
    "y_test_hat_rf = gb_final.predict(X_test)\n",
    "\n",
    "# Obtain and check mse on test data. Is it lower or higher than the RF?\n",
    "acc_rf = accuracy_score(y_test, y_test_hat_rf)\n",
    "print(f'Random Forest achieved test accuracy of {round(acc_rf * 100, 2)}%.')\n",
    "\n",
    "svm_model = svm.SVC() # initialize SVM\n",
    "\n",
    "#  fit on both train. and val. data\n",
    "svm_model.fit(np.concatenate([Z_train, Z_val]), np.concatenate([y_train, y_val]))\n",
    "\n",
    "# predict on test data\n",
    "y_test_hat_svm = svm_model.predict(Z_test)\n",
    "\n",
    "acc_svm = accuracy_score(y_test, y_test_hat_svm)\n",
    "print(f'SVM achieved test accuracy of {round(acc_svm * 100, 2)}%.')\n",
    "\n",
    "# print(f'Optimized GB achieved MSE = {round(gb_optimized*100, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of boosting, RF, and SVM achieved test accuracy of 96.49%.\n"
     ]
    }
   ],
   "source": [
    "# Finally combine your predictions\n",
    "# (you do not have to change the code here, but you may want to try to improve beyond this method)\n",
    "\n",
    "# My notes\n",
    "# y_test_hat_gbt = ensemble.GradientBoostingClassifier()\n",
    "# y_test_hat_rf = ensemble.GradientBoostingClassifier()\n",
    "# y_test_hat_svm \n",
    "\n",
    "# WARNING: The below code for voting is only valid for 2 classes - DO NOT USE IT FOR CASES WITH MORE THAN 2 CLASSES\n",
    "y_test_hat_combined = np.c_[y_test_hat_gbt, y_test_hat_rf, y_test_hat_svm]\n",
    "y_test_hat_combined = np.round(np.sum(y_test_hat_combined, axis=1) / y_test_hat_combined.shape[1]).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_hat_combined)\n",
    "\n",
    "print(f'Ensemble of boosting, RF, and SVM achieved test accuracy of {round(acc * 100, 2)}%.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
