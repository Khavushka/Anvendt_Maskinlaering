{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Boosting for classification\n",
    "\n",
    "1. Use the **load_breast_cancer** data (remember to split your data into a train, validation, and test data). Using your training and validation data, optimize the parameters of your GradientBoostingClassifier. How well does your optimized model perform on the test data?\n",
    "1. Implement an RF and a SVM and use these as well (**note**: you may want to perform standardization for the SVM). How well do they perform on the test data? Try to \"vote\" using all three models (boosting, RF, and SVM) and select the class with the most votes. How well does your ensemble of all three models perform?\n",
    "\n",
    "**See slides for more details!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import ensemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Use `train_test_split` to split your data into a train and a test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use `train_test_split` to split your train data into a train and a validation  set.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Use the **load_breast_cancer** data (remember to split your data into a train, validation, and test data). Using your training and validation data, optimize the parameters of your GradientBoostingClassifier. How well does your optimized model perform on the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by ensuring we can just run an GBT without any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_current = ensemble.GradientBoostingClassifier()\n",
    "gbt_current.fit(X_train, y_train)\n",
    "y_val_hat = gbt_current.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_val_hat)\n",
    "\n",
    "print(f'Boosting with default settings has validation accuracy of {round(acc * 100, 2)}%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember you can try other stuff than these specific parameters.\n",
    "# Just here to get you started!\n",
    "\n",
    "n_estimators_list = []\n",
    "min_samples_split_list = []\n",
    "min_samples_leaf_list = []\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    for min_samples_split in min_samples_split_list:\n",
    "        for min_samples_leaf in min_samples_leaf_list:\n",
    "            gbt_current = ensemble.GradientBoostingClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                )\n",
    "            gbt_current.fit(X_train, y_train)\n",
    "            y_val_hat = gbt_current.predict(X_val)\n",
    "            acc = accuracy_score(y_val, y_val_hat)\n",
    "\n",
    "            results.append([acc, n_estimators, min_samples_split, min_samples_leaf])\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.columns = ['Accuracy', 'n_estimators', 'min_samples_split', 'min_samples_leaf']\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your final model\n",
    "\n",
    "# Use both training and validation data to fit it using np.concatenate (np.concatenate \"stacks\" the array like rbind in R)\n",
    "\n",
    "# Predict on test data\n",
    "\n",
    "# Obtain and check accuracy on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Implement an RF and a SVM and use these as well (**note**: you may want to perform standardization for the SVM). How well do they perform on the test data? Try to \"vote\" using all three models (boosting, RF, and SVM) and select the class with the most votes. How well does your ensemble of all three models perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "# Scale your data\n",
    "scaler = StandardScaler()\n",
    "Z_train = scaler.fit_transform(X_train)\n",
    "Z_val = scaler.transform(X_val)\n",
    "Z_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to optimize the settings if you want.\n",
    "# Then, you can do it here.\n",
    "# You can/may want to do this both for the RF and the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your final models\n",
    "\n",
    "\n",
    "# Use both training and validation data to fit them using np.concatenate (np.concatenate \"stacks\" the array like rbind in R)\n",
    "\n",
    "\n",
    "# Predict on test data\n",
    "\n",
    "\n",
    "# Obtain and check mse on test data. Is it lower or higher than the RF?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally combine your predictions\n",
    "# (you do not have to change the code here, but you may want to try to improve beyond this method)\n",
    "\n",
    "# WARNING: The below code for voting is only valid for 2 classes - DO NOT USE IT FOR CASES WITH MORE THAN 2 CLASSES\n",
    "y_test_hat_combined = np.c_[y_test_hat_gbt, y_test_hat_rf, y_test_hat_svm]\n",
    "y_test_hat_combined = np.round(np.sum(y_test_hat_combined, axis=1) / y_test_hat_combined.shape[1]).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_hat_combined)\n",
    "\n",
    "print(f'Ensemble of boosting, RF, and SVM achieved test accuracy of {round(acc * 100, 2)}%.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
